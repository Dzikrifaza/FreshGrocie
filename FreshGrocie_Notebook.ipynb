{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dzikrifaza/FreshGrocie/blob/ml_dev/FreshGrocie_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library"
      ],
      "metadata": {
        "id": "ZvM9uJeihEUb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOJ-jD8Vjidl",
        "outputId": "8772f8ef-4e5b-4200-eb88-37d96b4afd96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0uthD4h_j842"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "from keras.utils import Sequence\n",
        "\n",
        "import opendatasets as od\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import splitfolders \n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLEwN7DJNKwh"
      },
      "source": [
        "#Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yCVqoBGkEjm",
        "outputId": "0aed67d4-04ed-46c2-a82c-74ab0f382b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: dianaayum151dsy1844\n",
            "Your Kaggle Key: ··········\n",
            "Downloading fruits-and-vegetables-dataset.zip to ./fruits-and-vegetables-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 870M/870M [00:36<00:00, 24.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/muhriddinmuxiddinov/fruits-and-vegetables-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "3ratgqxLmp5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JvDhVPYLZezY",
        "outputId": "6bdcbb0d-a2f7-4070-9672-e99d7cb74c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 5997 files [00:01, 4373.38 files/s]\n"
          ]
        }
      ],
      "source": [
        "splitfolders.ratio('/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset(12000)/Fruits', \n",
        "                   output=\"/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Fruits/\", \n",
        "                   seed=42, \n",
        "                   ratio=(0.8,0.1,0.1), # training, val, test\n",
        "                   group_prefix=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitfolders.ratio('/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset(12000)/Vegetables', \n",
        "                   output=\"/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Vegetables\", \n",
        "                   seed=42, \n",
        "                   ratio=(0.8,0.1,0.1), # training, val, test\n",
        "                   group_prefix=None)"
      ],
      "metadata": {
        "id": "YzRa0lHRnLqU",
        "outputId": "271eac51-c6ae-48cd-8f88-3b7e0e854a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 6003 files [00:02, 2821.32 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge Train and Test Data for Fruits and Vegetables "
      ],
      "metadata": {
        "id": "i1uQ35DVXwak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funtion for moving the directory"
      ],
      "metadata": {
        "id": "2m6xAtq2gjOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_dir(src, dest, dir_list):\n",
        "  for dir_ in dir_list:\n",
        "    source = os.path.join(src, dir_)\n",
        "\n",
        "    if os.path.isdir(source):\n",
        "        shutil.move(source, dest)"
      ],
      "metadata": {
        "id": "kzTw26JjgEcg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_subdir(source, dest, subdir):\n",
        "  if not os.path.isdir(dest):\n",
        "      os.mkdir(dest)\n",
        "\n",
        "  src_path = os.path.join(source, subdir)\n",
        "  for item_list in os.listdir(src_path):\n",
        "      src = os.path.join(src_path, item_list)\n",
        "      dst = os.path.join(dest, item_list)\n",
        "      shutil.move(src, dst)\n",
        "\n",
        "  os.rmdir(src_path)"
      ],
      "metadata": {
        "id": "39RWqGJ9g1wF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Data:\n",
        "\n",
        "Taking the data from fruits and vegetables and combine them into 1 training data"
      ],
      "metadata": {
        "id": "Jn0ERAnRqnD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Fruits'\n",
        "\n",
        "dir_list = ['train']\n",
        "\n",
        "dest_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset'\n",
        "dest = os.path.join(dest_path, 'train')\n",
        "\n",
        "move_dir(source_path, dest, dir_list)"
      ],
      "metadata": {
        "id": "VwnE5KLSqu2z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Vegetables'\n",
        "\n",
        "subdir = 'train'\n",
        "\n",
        "dest_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/train'\n",
        "\n",
        "move_subdir(source_path, dest_path, subdir)"
      ],
      "metadata": {
        "id": "26RjpPebqu20"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Data:\n",
        "\n",
        "Taking the data from fruits and vegetables and combine them into 1 validation data"
      ],
      "metadata": {
        "id": "HtsrTQywqFFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Fruits'\n",
        "\n",
        "dir_list = ['val']\n",
        "\n",
        "dest_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset'\n",
        "dest = os.path.join(dest_path, 'val')\n",
        "\n",
        "move_dir(source_path, dest, dir_list)"
      ],
      "metadata": {
        "id": "cC8ZyLLqcSzB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Vegetables'\n",
        "\n",
        "subdir = 'val'\n",
        "\n",
        "dest_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/val'\n",
        "\n",
        "move_subdir(source_path, dest_path, subdir)"
      ],
      "metadata": {
        "id": "jB4kwjkUmYbD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Data:\n",
        "\n",
        "Taking the data from fruits and vegetables and combine them into 1 testing data"
      ],
      "metadata": {
        "id": "RUU4Nq6nVOB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Fruits'\n",
        " \n",
        "dir_list = ['test']\n",
        " \n",
        "dest_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset'\n",
        "dest = os.path.join(dest_path, 'test')\n",
        "\n",
        "move_dir(source_path, dest, dir_list)"
      ],
      "metadata": {
        "id": "AB9fSzwFVOCG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/Vegetables'\n",
        "\n",
        "subdir = 'test'\n",
        "\n",
        "dest_path = '/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/test'\n",
        "\n",
        "move_subdir(source_path, dest_path, subdir)"
      ],
      "metadata": {
        "id": "675XhgNzVOCH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assign"
      ],
      "metadata": {
        "id": "4zj4a8v3XziF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_FileNames = os.path.join('/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/train')\n",
        "val_FileNames = os.path.join('/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/val')\n",
        "test_FileNames = os.path.join('/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset/test')"
      ],
      "metadata": {
        "id": "XgTE0_PEur4f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the RGBA"
      ],
      "metadata": {
        "id": "H0xb85xygdUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/fruits-and-vegetables-dataset/Fruits_Vegetables_Dataset\"\n",
        "\n",
        "for root, _, files in os.walk(directory):\n",
        "    for file in files:\n",
        "        if file.endswith(\".png\") or file.endswith(\".webp\"):\n",
        "            image_path = os.path.join(root, file)\n",
        "\n",
        "            with Image.open(image_path) as img:\n",
        "                if img.mode == \"P\" and \"transparency\" in img.info:\n",
        "                    img = img.convert(\"RGBA\")\n",
        "\n",
        "                save_path = os.path.splitext(image_path)[0] + \".png\"\n",
        "                if file.endswith(\".webp\"):\n",
        "                    save_path = os.path.splitext(image_path)[0] + \".jpg\"\n",
        "\n",
        "                img.save(save_path)"
      ],
      "metadata": {
        "id": "A5wwApx0gnoK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "ziHskJS1t8Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPmJcPw0WxXy",
        "outputId": "47a751a7-4621-491c-a3a9-875b3ca6ef90"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-28 08:41:42--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.128.128, 74.125.143.128, 173.194.69.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.128.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  26.4MB/s    in 3.2s    \n",
            "\n",
            "2023-05-28 08:41:45 (26.4 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "metadata": {
        "id": "K8BYN-KFWpBY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pre_trained(local_weights_file):\n",
        "  pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
        "                                  include_top = False, \n",
        "                                  weights = None,\n",
        "                                  pooling ='max',\n",
        "                                  classes = 20) \n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  return pre_trained_model"
      ],
      "metadata": {
        "id": "DIQ751lkW57M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = create_pre_trained(local_weights_file)"
      ],
      "metadata": {
        "id": "tBL5hNxfXJ5Q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_of_last_layer(pre_trained_model):\n",
        "  last_desired_layer = pre_trained_model.get_layer('mixed7')\n",
        "  last_output = last_desired_layer.output\n",
        "\n",
        "  return last_output"
      ],
      "metadata": {
        "id": "FBZ8DxZQXQvG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_output = output_of_last_layer(pre_trained_model)"
      ],
      "metadata": {
        "id": "sHFbj4VLZC2s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "def create_final_model(pre_trained_model, last_output):\n",
        "  x = layers.Flatten()(last_output)\n",
        "  x = layers.Dense(216, activation='relu')(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(20, activation='softmax')(x)        \n",
        "\n",
        "  model = Model(inputs=pre_trained_model.input, outputs=x)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hIF7dqboXqkL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_final_model(pre_trained_model, last_output)"
      ],
      "metadata": {
        "id": "Z5z3M1byaVvg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization"
      ],
      "metadata": {
        "id": "MCjib3xXgPmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                   brightness_range=(0.2, 0.8),\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_FileNames,  \n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_FileNames,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH0QiCzkjEVF",
        "outputId": "296c58c6-1414-4006-b9ec-245bc914c934"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9582 images belonging to 20 classes.\n",
            "Found 1192 images belonging to 20 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile and Train the model"
      ],
      "metadata": {
        "id": "RO6MWP28cLJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.0025, momentum=0.9),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "eOophxPBBjOQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant for epochs\n",
        "# Train the model\n",
        "inception_v3 = model.fit(\n",
        "    train_generator,\n",
        "    batch_size = 20,\n",
        "    steps_per_epoch = 400,  # 9582 images = batch_size * steps \n",
        "    epochs= 10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50,  # 2408 images = batch_size * steps\n",
        "    verbose=2)"
      ],
      "metadata": {
        "id": "3__-eypdDh4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ddac04-451d-4e3e-aefe-6a80798c6556"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "400/400 - 113s - loss: 1.4543 - accuracy: 0.5418 - val_loss: 0.6458 - val_accuracy: 0.7720 - 113s/epoch - 283ms/step\n",
            "Epoch 2/10\n",
            "400/400 - 98s - loss: 0.8467 - accuracy: 0.7273 - val_loss: 0.5561 - val_accuracy: 0.8210 - 98s/epoch - 245ms/step\n",
            "Epoch 3/10\n",
            "400/400 - 93s - loss: 0.7244 - accuracy: 0.7575 - val_loss: 0.4796 - val_accuracy: 0.8550 - 93s/epoch - 232ms/step\n",
            "Epoch 4/10\n",
            "400/400 - 94s - loss: 0.6523 - accuracy: 0.7869 - val_loss: 0.4156 - val_accuracy: 0.8660 - 94s/epoch - 236ms/step\n",
            "Epoch 5/10\n",
            "400/400 - 92s - loss: 0.5899 - accuracy: 0.8074 - val_loss: 0.4127 - val_accuracy: 0.8560 - 92s/epoch - 231ms/step\n",
            "Epoch 6/10\n",
            "400/400 - 98s - loss: 0.5569 - accuracy: 0.8162 - val_loss: 0.3694 - val_accuracy: 0.8780 - 98s/epoch - 244ms/step\n",
            "Epoch 7/10\n",
            "400/400 - 93s - loss: 0.5346 - accuracy: 0.8216 - val_loss: 0.3380 - val_accuracy: 0.8800 - 93s/epoch - 233ms/step\n",
            "Epoch 8/10\n",
            "400/400 - 94s - loss: 0.5133 - accuracy: 0.8300 - val_loss: 0.3355 - val_accuracy: 0.8840 - 94s/epoch - 234ms/step\n",
            "Epoch 9/10\n",
            "400/400 - 93s - loss: 0.4940 - accuracy: 0.8307 - val_loss: 0.3115 - val_accuracy: 0.8970 - 93s/epoch - 232ms/step\n",
            "Epoch 10/10\n",
            "400/400 - 98s - loss: 0.4645 - accuracy: 0.8441 - val_loss: 0.3272 - val_accuracy: 0.8900 - 98s/epoch - 245ms/step\n"
          ]
        }
      ]
    }
  ]
}